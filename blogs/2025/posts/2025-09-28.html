<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <title>2025 09 28</title>
    <link rel="stylesheet" href="post.css" />
    
    <!-- MathJax Configuration -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            macros: {
                "coloneq": "\\mathrel{\\mathop:}=",
                "RR": "\\mathbb{R}",
                "CC": "\\mathbb{C}",
                "NN": "\\mathbb{N}",
                "PP": "\\mathbb{P}",
                "E": "\\mathbb{E}",
                "D": "\\mathbb{D}",
                "P": "\\mathcal{P}",
                "H": "\\mathcal{H}",
                "X": "\\mathcal{X}",
                "B": "\\mathcal{B}",
                "F": "\\mathcal{F}",
                "M": "\\mathbb{M}",
                "var": "\\mathrm{var}",
                "Var": "\\mathrm{Var}",
                "cov": "\\mathrm{cov}",
                "Cov": "\\mathrm{Cov}",
                "corr": "\\mathrm{corr}",
                "Corr": "\\mathrm{Corr}",
                "pr": "\\mathrm{Pr}",
                "prob": "\\mathrm{Pr}",
                "normal": "\\mathcal{N}",
                "MSE": "\\mathrm{MSE}",
                "KL": "\\mathrm{KL}",
                "V": "\\mathbf",
                "vx": "\\boldsymbol{x}",
                "trace": "\\mathrm{trace}",
                "diag": "\\mathrm{diag}",
                "tt": "^\\top",
                "df": "\\mathrm{d}",
                "d": "\\mathrm{d}",
                "dt": "\\mathrm{d}t",
                "dW": "\\mathrm{d}W",
                "dno": "\\mathrm{d}",
                "dd": "\\nabla",
                "tdd": "\\tilde{\\nabla}",
                "ddt": "\\frac{\\mathrm{d}}{\\mathrm{d} t}",
                "div": "\\nabla\\cdot",
                "ind": "\\mathbb{I}",
                "sign": "\\mathrm{sign}",
                "cd": "\\mid",
                "la": "\\langle",
                "ra": "\\rangle",
                "sumstein": "\\mathcal{T}",
                "stein": "\\mathcal{A}",
                "score": "\\boldsymbol{s}",
                "ff": "\\boldsymbol{\\phi}",
                "eng": "E",
                "hattheta": "\\hat{\\theta}",
                "cc": "\\theta",
                "kernel": "\\mathbf{k}",
                "datai": "^{(i)}",
                "workingDate": "\\textsc{<YEAR> $|$ <MONTH_NAME> $|$ <DAY>}",
                "userName": "<AUTHOR>",
                "institution": "<INSTITUTION>",
                "diaryTitle": "<DIARY_TITLE>",
                "myb": ["\\mbox{\\boldmath$#1$}", 1],
                "eqnref": ["Eqn.~\\eqref{#1}", 1],
                "figref": ["Fig.~\\ref{#1}", 1],
                "here": ["\\url{#1}", 1],
                "Qiang": ["\\textcolor{red}{#1}\\\\", 1],
                "qiang": ["\\textcolor{red}{#1}", 1],
                "Todo": ["\\textcolor{red}{TODO: #1}\\\\", 1],
                "todo": ["\\textcolor{gray}{TODO: #1}", 1],
                "remark": ["\\paragraph{Remark}#1", 1],
                "red": ["\\textcolor{red}{#1}", 1],
                "med": ["\\textcolor{magenta}{#1}", 1],
                "blue": ["\\textcolor{blue}{#1}", 1],
                "gray": ["\\textcolor{gray}{#1}", 1],
                "green": ["\\textcolor{green}{#1}", 1],
                "myempty": ["", 1],
                "e": ["\\mathbb{E}\\left[#1\\right]", 1],
                "vv": ["\\boldsymbol{#1}", 1],
                "norm": ["\\left\\lVert#1\\right\\rVert", 1],
                "abs": ["\\left\\lvert#1\\right\\rvert", 1],
                "neib": ["\\partial{#1}", 1],
                "set": ["\\mathcal{#1}", 1],
                "funcset": ["\\mathcal{#1}", 1],
                "opt": ["\\mathcal{#1}", 1],
                "greedy": ["\\mathrm{Greedy}\\{#1\\}", 1],
                "clickhere": ["\\href{run:#1}{Click here}", 1],
                "showlabels": ["", 1],
                "myf": ["\\frac{\\displaystyle #1}{\\displaystyle #2}", 2],
                "myp": ["\\frac{\\displaystyle \\partial #1}{\\displaystyle \\partial  #2}", 2],
                "f": ["\\frac{#1}{#2}", 2],
                "definecolorbox": ["%
  \\expandafter\\declaretheoremstyle\\expandafter[%
    headfont=\\bfseries\\sffamily\\color{#1!70!black}, 
    bodyfont=\\normalfont,
    mdframed={%
      linewidth=2pt,
      rightline=false, 
      topline=false, 
      bottomline=false,
      linecolor=#1, 
      backgroundcolor=#1!5,
    }
  ]{#2}%
", 2]
            }
        },
        options: {
            ignoreHtmlClass: "tex2jax_ignore",
            processHtmlClass: "tex2jax_process"
        }
    };
    </script>
    <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body>
    <p><a href="run:2025-09-28.tex">September 28</a></p>
<h1 id="lecture-policy-gradient">Lecture: Policy Gradient</h1>
<p>For simplicity, let us first consider a one-step decision-making
process (also known as a contextual bandit).</p>
<h4 id="problem-setup">Problem Setup</h4>
<p>Let <span class="math inline">\(s \in \set S\)</span> be the state
vector of the environment, and let <span class="math inline">\(a \in
\set A\)</span> be the action taken by the agent. The reward function is
denoted by <span class="math inline">\(r(a,s)\)</span>. The agentâ€™s
behavior is characterized by a policy, which is conveniently represented
as a conditional probability distribution, <span
class="math inline">\(\pi(a \mid s)\)</span>, specifying the probability
of selecting each action <span class="math inline">\(a\)</span> given a
state <span class="math inline">\(s\)</span>. The objective is to find
the optimal policy <span class="math inline">\(\pi^*\)</span> that
maximizes the expected reward: <span class="math display">\[%R(\pi \mid
s)
R(\pi) = \mathbb{E}_{s\sim p_0}\mathbb{E}_{a\sim \pi(\cdot | s)} [r(a,s)] = \sum_{s,a}
p_\pi(a, s) r(a,s),\]</span> where <span
class="math inline">\(p_0\)</span> denotes the distribution of states,
and we have <span class="math inline">\(p_{\pi}(a,s)\)</span> denotes
the joint distribution of <span class="math inline">\((a,s)\)</span>
when policy <span class="math inline">\(\pi\)</span> is used: <span
class="math display">\[p_\pi(a,s) = \pi(a\mid s) p_0(s).\]</span>
Typically, <span class="math inline">\(p_0\)</span> is unknown to us,
but observed through a collection of data <span
class="math inline">\(\{s^{(i)}\}\)</span> drawn from <span
class="math inline">\(p_0\)</span>.</p>
<div class="center">

</div>
<h4 id="policy-gradient">Policy Gradient</h4>
<p>In practice, the policy <span class="math inline">\(\pi(a \mid s) =
\pi_\theta(a \mid s)\)</span> is parameterized by a function with
parameter <span class="math inline">\(\theta\)</span>. Assume the
state-action space <span class="math inline">\(\mathcal S \times
\mathcal A\)</span> is finite or accountable, the gradient of the
expected reward with respect to <span
class="math inline">\(\theta\)</span> is <span
class="math display">\[\begin{align}
\nabla_\theta R(\textcolor{blue}{\pi_\theta})
&amp; =  \nabla_\theta \left ( \sum_{s,a} p_0(s) \textcolor{blue}{\pi_\theta(a \mid
s)} r(a, s)  \right ) \notag \\
&amp; = \sum_{s,a} p_0(s) \textcolor{blue}{\nabla_\theta \pi_\theta(a \mid s)} r(a,
s)  \notag \\
&amp; = \sum_{s,a} p_0(s) \textcolor{blue}{\pi_\theta(a \mid s)}\textcolor{blue}{ \frac{
\nabla_\theta \pi_\theta(a \mid s) }{\pi_\theta(a \mid s)} } r(a, s)
\notag \\
&amp; = \mathbb{E}_{\textcolor{blue}{p_{\pi_\theta}}} \left[ r(a, s) \textcolor{blue}{\nabla_\theta
\log \pi_\theta(a \mid s)} \right], \notag
\end{align}\]</span> where the parts depending on <span
class="math inline">\(\theta\)</span> are highlighted, and we use the
log-derivative trick <span class="math inline">\(\nabla_\theta \log \pi =
\nabla_\theta \pi_\theta / \pi_\theta\)</span>. This allows us to
express the derivative of an expectation with respect to a distribution
as the expectation of the reward weighted by <span
class="math inline">\(\nabla_\theta \log \pi_\theta(a \mid s)\)</span>.</p>
<h4 id="reward-baseline">Reward Baseline</h4>
<p>For any policy <span class="math inline">\(\pi^\theta\)</span>, it is
easy to prove the following identity: <span
class="math display">\[\mathbb{E}_{a\sim \pi_\theta(\cdot|s)}[\nabla_\theta \log
\pi_\theta(a\mid s)] =0,~~~~~ \forall s,~\theta.\]</span></p>
<p>Hence, we can generalize the gradient formula above to <span
class="math display">\[\begin{align}
\label{equ:pcgrad2}
\nabla_\theta  R(\pi_\theta )
&amp; = \mathbb{E}_{p_{\pi_\theta}}  \left [  (r(a,s) -
\textcolor{magenta}{v(s)})  \textcolor{blue}{\nabla_\theta  \log  \pi_\theta (a\mid s)}  \right ],
\end{align}\]</span> where <span class="math inline">\(v(s)\)</span> is
<em>any</em> function that does not depend on the action <span
class="math inline">\(a\)</span>. The choice of <span
class="math inline">\(v\)</span> does not alter the expectation of the
formula, but a proper choice of <span class="math inline">\(v\)</span>
can reduce the variance of the mean estimation given empirical
observations.</p>
<p>One common choice of baseline is <span class="math inline">\(v(s) =
\mathbb{E}_{a\sim \pi_\theta(\cdot|s)}[r(a,s)]\)</span>, in which case the
difference <span class="math inline">\(r(a,s) - v(s)\)</span> is known
as the <em>advantage function</em>: <span class="math display">\[A(a, s)
= r(a,s) - v(s).\]</span> A positive value <span
class="math inline">\(A(a,s) &gt; 0\)</span> (respectively, negative
<span class="math inline">\(&lt;0\)</span>) indicates that the action
<span class="math inline">\(a\)</span> performs above (respectively,
below) the average.</p>
<h4 id="on-policy-estimation">On-Policy Estimation</h4>
<p>In practice, given observations <span
class="math inline">\(\{s^{(i)}\}\)</span> drawn from <span
class="math inline">\(p_0\)</span> and <span
class="math inline">\(a^{(i)} \sim \pi^\theta(\cdot|s^{(i)})\)</span>
drawn from the ongoing policy <span
class="math inline">\(\pi^\theta\)</span>, the gradient can be estimated
by <span class="math display">\[\begin{align}
\label{equ:onpolicyestimate}
\nabla_\theta  R(\pi_\theta )
\approx \frac{1}{n}\sum_{i=1}^n  
A^{(i)} \nabla_\theta  \log  \pi_\theta (a^{(i)} \mid s^{(i)}),~~~~~~
A^{(i)} =  A(a^{(i)}, s^{(i)}).
\end{align}\]</span> The gradient ascent update, also known as
REINFORCE, <span class="math display">\[\theta\gets \theta + \epsilon
\nabla_\theta  R(\pi_\theta ).\]</span> Intuitively, the gradient <span
class="math inline">\(\nabla_\theta \log \pi_\theta (a^{(i)} \mid
s^{(i)})\)</span> for data points <span class="math inline">\((a^{(i)},
s^{(i)})\)</span> with positive advantages <span
class="math inline">\(A^{(i)} &gt; 0\)</span> is positively weighted,
increasing the probability <span class="math inline">\(\pi_\theta(a \mid
s)\)</span> during gradient ascent. Conversely, for data points with
negative advantages, the gradient is negatively weighted, decreasing the
probability.</p>
<h4 id="connection-to-weighted-mle">Connection to Weighted MLE</h4>
<p>Assuming the data <span class="math inline">\(\{a^{(i)},
s^{(i)}\}\)</span> are fixed, the policy gradient <span
class="math inline">\(\nabla_\theta R(\pi_\theta)\)</span> coincides with
the gradient of the <span
class="math inline">\(A^{(i)}\)</span>-weighted log-likelihood function:
<span class="math display">\[\ell(\pi_\theta) = \frac{1}{n} \sum_{i=1}^n
A^{(i)} \log \pi_\theta(a^{(i)} \mid s^{(i)}).\]</span> This aligns with
the intuition of maximizing <span class="math inline">\(\log \pi(a \mid
s)\)</span> for data points with large (and positive) <span
class="math inline">\(A^{(i)}\)</span>, while minimizing the
log-likelihood for negative <span
class="math inline">\(A^{(i)}\)</span>.</p>
<p>The key difference, however, is that the data <span
class="math inline">\(\{a^{(i)}, s^{(i)}\}\)</span> are drawn from the
policy <span class="math inline">\(\pi_\theta\)</span> and thus depend
on the current parameter <span class="math inline">\(\theta\)</span>. As
<span class="math inline">\(\theta\)</span> is updated during policy
optimization, the data must either be regenerated or reweighted to
reflect changes in <span class="math inline">\(\pi_\theta\)</span>. In
contrast, maximum likelihood estimation assumes the data are fixed.</p>
<p>Hence, policy optimization can be viewed as an adaptively weighted
maximum likelihood estimation (MLE), where the weights (<span
class="math inline">\(A^{(i)}\)</span>) are adjusted across iterations
using newly generated data.</p>
<h4 id="off-policy-estimation-via-importance-sampling">Off-Policy
Estimation via Importance Sampling</h4>
<p>The gradient estimation in <a href="#equ:onpolicyestimate"
data-reference-type="eqref"
data-reference="equ:onpolicyestimate">[equ:onpolicyestimate]</a> is
<em>on-policy</em>, because the actions <span
class="math inline">\(a\)</span> must be drawn from the current policy
<span class="math inline">\(\pi_\theta(\cdot | s)\)</span>. This does
not yield efficient use of data as new actions must be rolled out once
the policy is updated, and data from different policies cannot be used.
In comparison, <em>off-policy</em> methods allow us to leverage actions
from different policies.</p>
<p>One common approach to off-policy estimation is importance sampling.
Assume that the data <span class="math inline">\(\{a^{(i)},
s^{(i)}\}\)</span> are drawn from a <em>behavior policy</em> <span
class="math inline">\(\pi^{\mathtt{data}}\)</span>. Using importance
sampling, we have</p>
<p><span class="math display">\[\begin{align*}
\nabla_\theta R(\pi_\theta)  
&amp; = \mathbb{E}_{p_{\pi^{\texttt{data}}}}
\left[\frac{\pi_\theta(a|s)}{\pi^{\mathtt{data}}(a|s)} A(a,s)
\nabla_\theta \log \pi_\theta (a \mid s) \right] \\
&amp; \approx \frac{1}{z} \sum_{i=1}^n
w^{(i)} A^{(i)}  \nabla_\theta \log \pi_\theta (a^{(i)} \mid s^{(i)}),
~~~~~
\end{align*}\]</span> where <span class="math inline">\(w^{(i)}\)</span>
is the importance weight: <span class="math display">\[w^{(i)} =
\frac{\pi_\theta(a^{(i)}|s^{(i)})}{\pi^{\mathtt{data}}(a^{(i)}|s^{(i)})},\]</span>
and the normalization constant <span class="math inline">\(z\)</span> is
often taken as <span class="math inline">\(z = \sum_i
w^{(i)}\)</span>.</p>
<div class="cremark">
<p>In RL, <em>on-policy methods</em> update the policy using data
collected from the current policy itself. <em>Off-policy methods</em>
learn from data collected under a different policy, allowing reuse of
past experiences and improving sample efficiency, but at the cost of a
distribution mismatch between the behavior policy <span
class="math inline">\(\pi^{\text{data}}\)</span> and the target policy
<span class="math inline">\(\pi_\theta\)</span>. <em>Offline RL</em> is
the case of off-policy learning where training relies entirely on a
fixed dataset with no further interaction.</p>
</div>
   
</body>
</html>
